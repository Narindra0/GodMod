from src.core import config
from src.scrapers import scraper_results
from src.scrapers import scraper_ranking
from src.scrapers import scraper_odds
from src.analysis import intelligence
from src.core import database
from src.core import utils
from src.core import archive
from src.core.database import get_db_connection
import time
import logging
import re
from playwright.sync_api import sync_playwright

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Variable globale pour tracker l'√©tat pr√©c√©dent de la s√©lection am√©lior√©e
_last_selection_state = None

def determiner_prochaine_journee():
    """D√©termine le num√©ro de la prochaine journ√©e √† pr√©dire via les r√©sultats."""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT MAX(journee) FROM resultats")
            derniere_journee = cursor.fetchone()[0] or 0
            return derniere_journee + 1
    except Exception as e:
        logger.error(f"Erreur lors de la d√©termination de la prochaine journ√©e : {e}", exc_info=True)
        return 1

def scraping_initial(page_results, page_ranking, force_same_session=False):
    """Effectue un scraping initial au d√©marrage pour constituer la base de donn√©es."""
    print("\n" + "="*50)
    print("üöÄ SCRAPING INITIAL")
    print("="*50)
    
    if force_same_session:
        print("‚ÑπÔ∏è Mode 'M√™me Session' forc√© par l'utilisateur. D√©tection de changement de session D√âSACTIV√âE.")
    
    try:
        print("üîé Analyse pr√©ventive de la session...")
        # On attend un peu que le contenu soit l√† (le scraper fera un wait plus strict ensuite)
        try:
            page_results.wait_for_selector("text=/Journ√©e \\d+/", timeout=5000)
        except:
            pass # Si pas trouv√©, le scraper g√©rera ou la page est vide

        # On r√©cup√®re les num√©ros de journ√©es affich√©s
        headers = page_results.locator("text=/Journ√©e \\d+/").all_inner_texts()
        days_on_page = []
        for h in headers:
            m = re.search(r"Journ√©e\s+(\d+)", h)
            if m:
                days_on_page.append(int(m.group(1)))
        
        if days_on_page:
            # On prend la plus GRANDE journ√©e trouv√©e pour √©viter les faux resets avec des anciennes journ√©es
            # Ex: Si on a J12, J13, J14, J15 et qu'on est √† J14, prendre J12 causerait un reset.
            # On prend la plus GRANDE journ√©e trouv√©e pour √©viter les faux resets avec des anciennes journ√©es
            max_day = max(days_on_page)
            
            # On ne v√©rifie le changement de session QUE si l'utilisateur n'a pas forc√© "M√™me Session"
            if not force_same_session and archive.detecter_nouvelle_session(max_day):
                print(f"üîÑ D√©tection changement de session via J{max_day} sur la page r√©sultats.")
                print("\n" + "="*50)
                print("üóÇÔ∏è NOUVELLE SESSION D√âTECT√âE AU D√âMARRAGE !")
                print("="*50)
                fichier = archive.archiver_session()
                
                if fichier:
                    print(f"‚úÖ Session pr√©c√©dente archiv√©e : {fichier}")
                    archive.reinitialiser_tables_session()
                else:
                    print("‚ùå √âCHEC Archivage. Reset annul√© par s√©curit√©.")
                print("="*50 + "\n")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Warning: Echec d√©tection session pr√©-scraping (non bloquant): {e}")
    # -------------------------------------------------------------

    print("R√©cup√©ration des donn√©es historiques...")
    
    scraper_results.extraire_donnees_resultats(page_results)
    scraper_ranking.extraire_donnees_classement(page_ranking)
    
    print("‚úÖ Scraping initial termin√©. Base de donn√©es pr√™te.")
    print("="*50 + "\n")

def executer_cycle(p, browser, page_matches, page_results, page_ranking):
    """Ex√©cute un cycle complet avec synchronisation s√©quentielle J+1."""
    print("\n--- Nouveau Cycle d'Analyse ---")
    
    # 1. Attente du moment valide (Timer, LIVE 5s, et S√©quence J-1)
    # utils.wait_for_valid_cycle g√®re maintenant la v√©rification de la pr√©sence de J-1
    if not utils.wait_for_valid_cycle(page_matches, page_results, page_ranking):
        return False

    # 1.5 D√©tection de nouvelle session avant scraping
    journee_site = utils.get_journee_from_page(page_matches)
    if archive.detecter_nouvelle_session(journee_site):
        print("\n" + "="*50)
        print("üóÇÔ∏è NOUVELLE SESSION D√âTECT√âE !")
        print("="*50)
        fichier = archive.archiver_session()
        
        if fichier:
            print(f"‚úÖ Session pr√©c√©dente archiv√©e : {fichier}")
            archive.reinitialiser_tables_session()
        else:
            print("‚ùå √âCHEC Archivage. La r√©initialisation est ANNUL√âE par s√©curit√© pour ne pas perdre de donn√©es.")
            
        print("="*50 + "\n")

    # M√©morisation de l'√©tat avant scraping
    ancienne_max_journee = determiner_prochaine_journee() - 1

    # 2. Scraping S√©quentiel (Playwright n'est pas thread-safe)
    # Note: Playwright utilise des greenlets qui ne peuvent pas √™tre partag√©s entre threads
    print("Demarrage du scraping (Tentative 1)...")
    try:
        scraper_results.extraire_donnees_resultats(page_results)
        logger.info("‚úÖ R√©sultats termin√© avec succ√®s")
    except Exception as e:
        logger.error(f"‚ùå Erreur dans le scraper R√©sultats: {e}", exc_info=True)
        print(f"‚ùå Erreur dans le scraper R√©sultats: {e}")
    
    try:
        scraper_ranking.extraire_donnees_classement(page_ranking)
        logger.info("‚úÖ Classement termin√© avec succ√®s")
    except Exception as e:
        logger.error(f"‚ùå Erreur dans le scraper Classement: {e}", exc_info=True)
        print(f"‚ùå Erreur dans le scraper Classement: {e}")
    
    try:
        scraper_odds.extraire_donnees_cotes(page_matches)
        logger.info("‚úÖ Cotes termin√© avec succ√®s")
    except Exception as e:
        logger.error(f"‚ùå Erreur dans le scraper Cotes: {e}", exc_info=True)
        print(f"‚ùå Erreur dans le scraper Cotes: {e}")
    
    # 2.5 V√©rification et Re-Scraping si n√©cessaire
    nouvelle_max_journee = determiner_prochaine_journee() - 1
    
    if nouvelle_max_journee == ancienne_max_journee:
        print(f"‚ö†Ô∏è Warning : Aucune nouvelle journ√©e d√©tect√©e (Max J{ancienne_max_journee}).")
        print("‚è≥ Le site n'a peut-√™tre pas encore affich√© les r√©sultats. Attente de 15s avant retentive...")
        
        time.sleep(15)
        page_results.reload(wait_until="domcontentloaded")
        
        # Tentative 2
        print("üîÑ Demarrage du scraping (Tentative 2)...")
        scraper_results.extraire_donnees_resultats(page_results)
        
        # On ne re-scrape le classement/cotes que si les r√©sultats sont l√†, mais pour simplifier on peut ou non
        # Ici on re-tente surtout les r√©sultats car c'est le d√©clencheur
        
        nouvelle_max_journee_v2 = determiner_prochaine_journee() - 1
        if nouvelle_max_journee_v2 > ancienne_max_journee:
            print(f"‚úÖ Succ√®s Tentative 2 : Donn√©es r√©cup√©r√©es (J{nouvelle_max_journee_v2}) !")
        else:
            print("‚ùå Echec Tentative 2 : Toujours pas de nouvelles donn√©es. On passe au cycle suivant.")
    else:
        print(f"‚úÖ Donn√©es r√©cup√©r√©es avec succ√®s (J{nouvelle_max_journee}).")

    # 3. Validation des pr√©dictions
    intelligence.mettre_a_jour_scoring()

    # 4. Intelligence et Pr√©diction
    journee = determiner_prochaine_journee()
    print(f"Analyse pour la Journee {journee}...")
    
    # Recharger le config pour d√©tecter les changements depuis le dashboard
    import importlib
    import sys
    global _last_selection_state
    
    if 'src.core.config' in sys.modules:
        importlib.reload(sys.modules['src.core.config'])
        # R√©importer config pour avoir la valeur √† jour
        from src.core import config
        
        # D√©tecter le changement de phase et afficher le message
        current_state = config.USE_SELECTION_AMELIOREE
        
        # Initialiser l'√©tat au premier chargement
        if _last_selection_state is None:
            _last_selection_state = current_state
            # Afficher l'√©tat initial si Phase 3 est active
            if current_state:
                print("="*60)
                print("   üß† INTELLIGENCE ACTIVE")
                print("="*60)
                print("   ‚úÖ Mode Intelligence Compl√®te (Phase 3)")
                print("   ‚Ü≥ 7 facteurs d'analyse pond√©r√©s")
                print("   ‚Ü≥ D√©tection automatique des pi√®ges")
                print("   ‚Ü≥ Analyse des patterns historiques")
                print("="*60 + "\n")
            else:
                print("="*60)
                print("   ‚ÑπÔ∏è MODE STANDARD")
                print("="*60)
                print("   ‚Ü≥ Calcul simple : Classement + Forme")
                print("="*60 + "\n")
        # D√©tecter les changements ult√©rieurs
        elif _last_selection_state != current_state:
            print("\n" + "="*60)
            if current_state:
                print("   üîÑ CHANGEMENT DE MODE D√âTECT√â")
                print("="*60)
                print("   ‚úÖ ACTIVATION : Mode Intelligence Compl√®te")
                print("   ‚Ü≥ Passage √† la Phase 3 (7 facteurs)")
                print("   ‚Ü≥ D√©tection des pi√®ges activ√©e")
                print("   ‚Ü≥ Analyse approfondie activ√©e")
            else:
                print("   üîÑ CHANGEMENT DE MODE D√âTECT√â")
                print("="*60)
                print("   ‚ÑπÔ∏è D√âSACTIVATION : Retour au Mode Standard")
                print("   ‚Ü≥ Calcul simple uniquement")
            print("="*60 + "\n")
            _last_selection_state = current_state
    else:
        from src.core import config
        if _last_selection_state is None:
            _last_selection_state = config.USE_SELECTION_AMELIOREE
            if config.USE_SELECTION_AMELIOREE:
                print("="*60)
                print("   üß† INTELLIGENCE ACTIVE")
                print("="*60)
                print("   ‚úÖ Mode Intelligence Compl√®te (Phase 3)")
                print("="*60 + "\n")
    
    # Phase 3 : Choix de la fonction de s√©lection selon la configuration
    if config.USE_SELECTION_AMELIOREE:
        selections = intelligence.selectionner_meilleurs_matchs_ameliore(journee)
    else:
        selections = intelligence.selectionner_meilleurs_matchs(journee)
    
    if selections:
        print(f"Succes : {len(selections)} predictions generees pour la Journee {journee}.")
    else:
        print("Info : Aucune prediction pour ce cycle.")
    
    return True

def main():
    print("="*60)
    print("   üöÄ SYST√àME GODMOD V2 - D√âMARRAGE")
    print("="*60)
    
    # Afficher le mode activ√©
    if config.USE_INTELLIGENCE_AMELIOREE and config.USE_SELECTION_AMELIOREE:
        print("‚úÖ MODE INTELLIGENT ACTIV√â")
        print("   ‚Ü≥ Phase 3 Compl√®te : 7 facteurs d'analyse")
        print("   ‚Ü≥ D√©tection des pi√®ges de cotes")
        print("   ‚Ü≥ Analyse des confrontations directes")
        print("   ‚Ü≥ Calcul du momentum des √©quipes")
    elif config.USE_INTELLIGENCE_AMELIOREE:
        print("‚ö†Ô∏è MODE INTERM√âDIAIRE ACTIV√â")
        print("   ‚Ü≥ Phase 2 : Calcul am√©lior√© avec fallback")
    else:
        print("‚ÑπÔ∏è MODE NORMAL ACTIV√â")
        print("   ‚Ü≥ Calcul simple : Classement + Forme")
    
    print("="*60 + "\n")
    
    # --- MODIFICATION : Demande manuelle de session ---
    print("\n" + "!"*60)
    print("   ‚ùì QUESTION UTILISATEUR")
    print("!"*60)
    choix_session = input("   Est-ce une nouvelle session (reset + archive) ? (y/n) : ").strip().lower()
    
    force_same_session = False
    
    if choix_session == 'y':
        print("\n" + "="*50)
        print("üóÇÔ∏è NOUVELLE SESSION FORC√âE PAR L'UTILISATEUR")
        print("="*50)
        fichier = archive.archiver_session()
        if fichier:
            print(f"‚úÖ Session pr√©c√©dente archiv√©e : {fichier}")
        else:
            print("‚ö†Ô∏è Pas d'archive cr√©√©e (peut-√™tre vide ou erreur).")
            
        archive.reinitialiser_tables_session()
        print("‚úÖ Tables r√©initialis√©es.")
        print("="*50 + "\n")
        
    elif choix_session == 'n':
        print("\n‚ÑπÔ∏è Mode 'M√™me Session' s√©lectionn√©. La d√©tection automatique sera d√©sactiv√©e pour ce d√©marrage.")
        force_same_session = True
    else:
        print("\n‚ö†Ô∏è R√©ponse non reconnue. Comportement par d√©faut (D√©tection automatique).")

    # R√©initialisation pour migration propre (ne touche pas aux donn√©es, juste structure)
    database.initialiser_db()
    
    dernier_scrap_time = 0
    INTERVALLE_MIN_SCRAP = 60 
    
    with sync_playwright() as p:
        browser, page_matches, page_results, page_ranking = utils.init_persistent_browser(p)
        
        # Scraping initial pour avoir une base de donn√©es d√®s le d√©part
        scraping_initial(page_results, page_ranking, force_same_session=force_same_session)
        
        try:
            while True:
                # On lance le cycle. La fonction wait_for_valid_cycle g√®re l'attente intelligente.
                executer_cycle(p, browser, page_matches, page_results, page_ranking)
                
                print("Fin du cycle. Reprise de la surveillance dans 5 secondes...")
                time.sleep(5)
                
        except KeyboardInterrupt:
            print("\nArret par l'utilisateur.")
        except Exception as e:
            logger.error(f"Erreur critique dans la boucle principale : {e}", exc_info=True)
            print(f"‚ùå Erreur critique : {e}")
        finally:
            try:
                browser.close()
            except Exception as e:
                logger.warning(f"Erreur lors de la fermeture du navigateur : {e}")
            print("Navigateur ferme. Fin.")

if __name__ == "__main__":
    main()
